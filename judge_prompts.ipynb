{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt = \"\"\"\n",
    "You are an impartial, unbiased judge for a hackathon. You will be critiquing hackathon projects against two criteria: innovation and curiosity. \n",
    "\n",
    "Innovation is defined as challenging the norm by analyzing existing solutions through testing and/or research, and then exploring ways to improve or reinvent the norm. A project that demonstrates innovation may improve an existing solution in a way that increases value (e.g., access, affordability, opportunity, empowerment), and/or invent a brand-new approach/solution that addresses the problem. Consider: how the team challenged commonly accepted ideas and what they did differently in their project. \n",
    "\n",
    "Curiosity is defined as being curious by asking questions that inspire growth and result in the artful application of scientific principles. A project that demonstrates curiosity may question existing limitations, discover an unaddressed problem, and/or is “outside the box” that leverages unique ideas and processes. Consider: how the team explored possibilities and something clever, beautiful, or unique about their project.\n",
    "\n",
    "Given a project description, in a neutral and objective tone, provide a one-sentence problem statement and a one-sentence project description. Next, using only the information from the description without adding interpretive commentary or value judgments, extract the aspects of the project that demonstrate alignment and/or misalignment with the innovation and curiosity criteria, if any.\n",
    "\n",
    "As the judge, you also have to evaluate how the projects meet rubric criteria. Using the following rubrics for innovation and curiosity, identify how the project either “Fully meets”, “Partially meets”, or “Does not meet” each level in the rubric. Include an explanation.\n",
    "\n",
    "Innovation level 1: The team developed a project but did not research or analyze existing solutions and no potential improvements was identified, 2: The team developed a project, and did not examine existing solutions for potential improvements, instead used an already known or given area of improvement, 3: The team developed a project, conducted research on existing solutions, but did not analyze current accepted solutions to identify strengths or weaknesses, and did not explore new solutions, 4: The team developed a project, conducted research on existing solutions, analyze current accepted solutions to identify strengths or weaknesses, but did not explore new or alternative solutions, 5: The team based their project on research and analyzed currently accepted solutions to identify strengths or weaknesses, and explored one or more new or alternative successful solutions.\n",
    "\n",
    "Curiosity level 1: The team did not ask questions that addressed their understanding of the project topic areas, 2: The team asked questions to better understand their area of interest, 3: The team worked together to identify knowledge gaps, asking unique or new questions to bridge gaps, 4: The team worked together to identify knowledge gaps, asking unique or new questions, and made a plan that explored answers to improve project outcomes, 5: The team worked together to identify knowledge gaps, asking unique or new questions, and acted on that plan to explore create to project outcomes. \n",
    "\n",
    "Let’s work this out in a step by step way to be sure we have the right answer.\n",
    "\"\"\"\n",
    "\n",
    "neutral_prompt = (\n",
    "    judge_prompt + \"\\n-Assessment style: your assessment is accurate and unbiased.\"\n",
    ")\n",
    "\n",
    "positive_prompt = (\n",
    "    judge_prompt\n",
    "    + \"\\n-Assessment style: your assessment is accurate but you are inclined to provide a favorable review.\"\n",
    ")\n",
    "\n",
    "negative_prompt = (\n",
    "    judge_prompt\n",
    "    + \"\\n-Assessment style: your assessment is accurate but you are inclined to provide a n unfavorable review.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "projects_df = pd.read_csv(\"projects_selected.csv\")\n",
    "rubric_df = pd.read_csv(\"devpost_creativity_rubric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv(\"project_descriptions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatOllama(model=\"llama3.2\", temperature=0.5)\n",
    "OPENAI_API_KEY = \"your key here\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.5, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "def pass_judgement(prompt, project):\n",
    "    messages = [\n",
    "        SystemMessage(content=prompt),\n",
    "        HumanMessage(content=f\"Please judge this project: {project}\"),\n",
    "    ]\n",
    "    output = model.invoke(input=messages)\n",
    "    return output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def gen_judgement_dict(projects_df):\n",
    "    output_dict = dict()\n",
    "    for idx, row in projects_df.iterrows():\n",
    "        t = time.time()\n",
    "        print(f\"Judging row {idx}\")\n",
    "        description = row[\"description\"]\n",
    "        output_dict[idx] = dict()\n",
    "        outputs = [\n",
    "            pass_judgement(prompt, description)\n",
    "            for prompt in [neutral_prompt, positive_prompt, negative_prompt]\n",
    "        ]\n",
    "        output_dict[idx][\"neutral\"] = outputs[0]\n",
    "        output_dict[idx][\"positive\"] = outputs[1]\n",
    "        output_dict[idx][\"negative\"] = outputs[2]\n",
    "        print(f\"Took {time.time() - t}s\")\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judging row 0\n",
      "Took 30.10392689704895s\n",
      "Judging row 1\n",
      "Took 24.70517086982727s\n",
      "Judging row 2\n",
      "Took 16.394521951675415s\n",
      "Judging row 3\n",
      "Took 31.047686100006104s\n",
      "Judging row 4\n",
      "Took 18.097491025924683s\n",
      "Judging row 5\n",
      "Took 15.246944189071655s\n",
      "Judging row 6\n",
      "Took 12.77750015258789s\n",
      "Judging row 7\n",
      "Took 22.60185980796814s\n",
      "Judging row 8\n",
      "Took 19.032674074172974s\n",
      "Judging row 9\n",
      "Took 21.2361741065979s\n",
      "Judging row 10\n",
      "Took 17.13142204284668s\n",
      "Judging row 11\n",
      "Took 20.743053197860718s\n"
     ]
    }
   ],
   "source": [
    "output_dict = gen_judgement_dict(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_dict = json.dumps(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"judgements_json.json\", \"w+\") as f:\n",
    "    json.dump(output_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
