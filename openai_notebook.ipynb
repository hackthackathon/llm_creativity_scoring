{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "# from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "def call_model(model, prompt):\n",
    "    response = model.invoke(\n",
    "        input=[\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=prompt),\n",
    "        ],\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "def generate_reworded_text(description, model, criterion, criterion_desc):\n",
    "    prompt = (\n",
    "        f\"Analyze the project description below and provide a JSON response that evaluates the project based on the '{criterion}' criteria. \"\n",
    "        f\"Format your response as a JSON object with the following structure:\\n\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"alignment\": [\\n'\n",
    "        '    \"specific aspect 1 that aligns with criteria\",\\n'\n",
    "        '    \"specific aspect 2 that aligns with criteria\"\\n'\n",
    "        \"  ],\\n\"\n",
    "        '  \"misalignment\": [\\n'\n",
    "        '    \"specific aspect 1 that misaligns with criteria\",\\n'\n",
    "        '    \"specific aspect 2 that misaligns with criteria\"\\n'\n",
    "        \"  ],\\n\"\n",
    "        '  \"scoring_evaluation\": {\\n'\n",
    "        '    \"level_3\": {\\n'\n",
    "        '      \"status\": \"fully met|partially met|did not meet\",\\n'\n",
    "        '      \"explanation\": \"brief explanation\"\\n'\n",
    "        \"    },\\n\"\n",
    "        '    \"level_2\": {\\n'\n",
    "        '      \"status\": \"fully met|partially met|did not meet\",\\n'\n",
    "        '      \"explanation\": \"brief explanation\"\\n'\n",
    "        \"    },\\n\"\n",
    "        '    \"level_1\": {\\n'\n",
    "        '      \"status\": \"fully met|partially met|did not meet\",\\n'\n",
    "        '      \"explanation\": \"brief explanation\"\\n'\n",
    "        \"    }\\n\"\n",
    "        \"  }\\n\"\n",
    "        \"}\\n\\n\"\n",
    "        f\"Use only the information from the description without adding interpretive commentary or value judgments. Extract specific aspects that demonstrate alignment or misalignment with the '{criterion}' criteria.\\n\\n\"\n",
    "        f\"Definition of {criterion}: {criterion_desc}\\n\\n\"\n",
    "        f\"Project Description: {description}\\n\\n\"\n",
    "        f\"Ensure your response is properly formatted JSON that can be parsed by standard JSON parsers.\"\n",
    "    )\n",
    "    return call_model(model, prompt)\n",
    "\n",
    "def generate_project_summary(model, description):\n",
    "        \"\"\"\n",
    "        Adds a new column to the DataFrame with a summary of the project, including\n",
    "        the Project Overview and Problem Statement.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the project description:\\n\\n\"\n",
    "            f\"Provide one sentence describing the project.\\n\"\n",
    "            f\"Provide one sentence stating the problem the project is trying to solve.\\n\\n\"\n",
    "            f\"Project Description: {description}\\n\\n\"\n",
    "            f\"Response:\"\n",
    "        )\n",
    "        return call_model(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER - rewrite this to accomodate the Azure model rate limits! :)\n",
    "def process_projects(projects_df, rubric_df, temperature=0.3):\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=temperature)\n",
    "    \n",
    "    # Extract rubric descriptions once outside the loop\n",
    "    innovation_desc = rubric_df[rubric_df[\"Criterion\"] == \"Innovation\"][\"Description\"].iloc[0]\n",
    "    curiosity_desc = rubric_df[rubric_df[\"Criterion\"] == \"Curiosity\"][\"Description\"].iloc[0]\n",
    "\n",
    "    for idx, row in projects_df.iterrows():\n",
    "        print(f\"Processing row {idx}\")\n",
    "        description = row['description']\n",
    "        \n",
    "        # Process all assessments for one description at once\n",
    "        try:\n",
    "            projects_df.at[idx, 'Project_Summary'] = generate_project_summary(model, description)\n",
    "            projects_df.at[idx, 'Innovation_Assessment'] = generate_reworded_text(\n",
    "                description, model, \"Innovation\", innovation_desc\n",
    "            )\n",
    "            projects_df.at[idx, 'Curiosity_Assessment'] = generate_reworded_text(\n",
    "                description, model, \"Curiosity\", curiosity_desc\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {str(e)}\")\n",
    "            continue\n",
    "                \n",
    "        print(f\"Row {idx} completed\")\n",
    "        \n",
    "        # Optional: Save intermediate results\n",
    "        # projects_df.to_csv('intermediate_results.csv')\n",
    "        \n",
    "    return projects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project descriptions and rubric descriptions. \n",
    "projects_df = pd.read_csv(\"projects_selected.csv\")\n",
    "rubric_df = pd.read_csv(\"devpost_creativity_rubric.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_projects_df.to_csv(\"openai_processed_projects_json.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
